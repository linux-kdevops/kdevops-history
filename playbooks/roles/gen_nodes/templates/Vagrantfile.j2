# -*- mode: ruby -*-
# # vi: set ft=ruby :

# This file should only be edited to *grow* support for new features.
# If you are not interested in that and are happy with what is provided,
# all you'd have to do is edit your respective project name space
# yaml file to set the hosts / playbooks you need to run.
#
# This file is automatically created for you from a jinja2 template file
# and parsed with ansible after extra_vars.yaml file is created. The source is
# playbooks/roles/gen_nodes/templates/Vagrantfile.j2

Vagrant.require_version ">= 1.6.0"

require 'json'
require 'yaml'
require 'fileutils'
require 'rbconfig'

@os = RbConfig::CONFIG['host_os']

# XXX: upstream libvirt enhancement needed
#
# Vagrant allows multiple provider code to be supported easily, however
# this logic assuems all provider setup is supported through configuration
# variables from the provider. This is not the case for libvirt. We need
# to create the qemu image, and the libvirt vagrant provider doesn't have
# support to create the images as virtual box does. We run the commands
# natively, however this also reveals that on execution path even code
# for other providers gets executed regardless of the provider you are
# using *unless* that code is using provider specific variables. Best we
# can do for now is detect your OS and set a local variable where we *do*
# run different local code paths depending on the target provider.
#
# Right now we make these assumptions:
#
# Your OS            Provider
# -------------------------------
# Linux              libirt
# Mac OS X           virtualbox
provider = "libvirt"

# Are we in a nested virtualized environment? For now we just
# do something different if we're not on bare metal. We may want
# to do something different later to optimize this further but
# we have to draw the line somewhere.
nested = %x(which systemd-detect-virtd > /dev/null || systemd-detect-virt) != "none"

case
when @os.downcase.include?('linux')
  provider = "libvirt"
when @os.downcase.include?('darwin')
  provider = "virtualbox"
else
  puts "You OS hasn't been tested yet, go add support and send a patch."
  exit
end

# We assume if you are in kdevops/vagrant/ your project namespace is kdevops
project_namespace = File.basename(File.dirname(Dir.getwd))

node_config          =  "kdevops_nodes.yaml"
node_config_override =  "kdevops_nodes_override.yaml"

config_data = YAML.load_file(node_config)

override_data = false
if File.file?(node_config_override)
  override_data = true
  config_data_override = YAML.load_file(node_config_override)
end

global_data = config_data['vagrant_global']
vagrant_boxes = config_data['vagrant_boxes']

if override_data
  if config_data_override['vagrant_global']
    global_data = config_data_override['vagrant_global']
  end
  if config_data_override['vagrant_boxes']
    vagrant_boxes = config_data_override['vagrant_boxes']
  end
end

supported_provider = case provider
  when "virtualbox" then true
  when "libvirt" then true
  else false
end

if ! supported_provider
  puts "Unsupported provider: #{provider} on " + RbConfig::CONFIG['host_os']
  puts "Consider adding support and send a patch"
  exit
end

qemu_group = global_data['libvirt_cfg']['qemu_group']
qemu_group_auto = global_data['libvirt_cfg']['qemu_group_auto']

if qemu_group_auto
  if File.exists?('/etc/debian_version')
    qemu_group = 'libvirt-qemu'
  else
    qemu_group = 'qemu'
  end
end

pool_path = global_data['storage_pool_path']
kdevops_pool_path = pool_path + '/kdevops/'

libvirt_session_public_network_dev = global_data['libvirt_cfg']['session_public_network_dev']

Vagrant.configure("2") do |config|
  vagrant_boxes.each do |server_data|
    # Using sync folders won't work for say openstack / aws / azure / gce, and
    # for that we'll use terraform, so best to allow whatever data we want to
    # sync be provisioned with ansible.
    config.vm.synced_folder './', '/vagrant', type: '9p', disabled: true, accessmode: "mapped", mount: false
    config.vm.define server_data["name"] do |srv|
      srv.vm.box = server_data['box'] ? server_data['box'] : global_data["box"]
      server_box_version = server_data['box_version'] ? server_data['box_version'] : global_data["box_version"]
      if server_box_version != ""
        srv.vm.box_version = server_box_version
      end
{% if libvirt_session %}
      # https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F
      # qemu:///session has a serious drawback: since the libvirtd instance
      # does not have sufficient privileges, the only out of the box network
      # option is qemu's usermode networking, which has nonobvious
      # limitations, so its usage is discouraged. More info on qemu networking
      # options: http://people.gnome.org/~markmc/qemu-networking.html
      srv.vm.network "public_network", :type => "user"
{% else %}
      if ! nested
        srv.vm.network "private_network", ip: server_data["ip"]
      end
{% endif %}
      host_name = server_data["name"]
      nvme_path = kdevops_pool_path + "/" + global_data['nvme_disk_path'] + "/#{host_name}"
      FileUtils.mkdir_p(nvme_path) unless File.exists?(nvme_path)
{% if virtualbox_provider %}
      srv.vm.provider "virtualbox" do |vb, override|
	if provider == "virtualbox" && ! global_data['virtualbox_cfg']['auto_update']
          # we'll need to run later: vagrant vbguest install
          config.vbguest.auto_update = false
        end
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        vb.memory = global_data["memory"]
        vb.cpus = global_data["cpus"]
        if global_data['nvme_disks']
          port = 0
          port_count = global_data['nvme_disks'].size()
          global_data['nvme_disks'].each do |key, value|
            size = value['size']
	    # XXX: the purpose of the nvme drive is unused for now but later we
	    # can let ansible provision it for our target mount paths
            purpose = key
            port_plus = port + 1
            nvme_disk_postfix = global_data['virtualbox_cfg']['nvme_disk_postfix']
            nvme_disk = nvme_path + "/nvme#{port}n#{port_plus}.#{nvme_disk_postfix}"

            # "Standard" provides a sparse file. That's what we want, we cheat
            # the OS and only use what we need. If you want the real file size
            # add a global config option and send a patch and justify it. I'd
            # like to hear about it. We use sparse files for libvirt as well
            # and should try to keep setup in sync.
            if (! File.file?(nvme_disk))
              vb.customize ["createmedium", "disk", "--filename", nvme_disk, "--variant", "Standard", "--format", nvme_disk_postfix.upcase, "--size", size]
            end
            # Virtualbox supports only one nvme controller... this will fail
            # unless you are a Virtualbox hacker adding support for this :)
            if global_data['virtualbox_cfg']['nvme_controller_per_disk']
              # https://www.virtualbox.org/manual/ch08.html#vboxmanage-storagectl
              # This command attaches, modifies, and removes a storage
              # controller. After this, virtual media can be attached to the
              # controller with the storageattach command.
              nvme_name = "nvme#{port}"
              if (! File.file?(nvme_disk))
                vb.customize ["storagectl", :id, "--name", "#{nvme_name}", "--add", "pcie", "--controller", "NVMe", "--portcount", 1, "--bootable", "off"]
              end
	      # Now attach the drive
	      vb.customize ["storageattach", :id, "--storagectl", "#{nvme_name}", "--type", "hdd", "--medium", nvme_disk, "--port", 0]
            else
              if (port == 0 && !File.file?(nvme_disk))
                vb.customize ["storagectl", :id, "--name", "nvme0", "--add", "pcie", "--controller", "NVMe", "--portcount", port + 1, "--bootable", "off"]
              end
	      vb.customize ["storageattach", :id, "--storagectl", "nvme0", "--type", "hdd", "--medium", nvme_disk, "--port", port]
            end

            if global_data['enable_sse4']
	      # Support for the SSE4.x instruction is required in some versions of VB.
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.1", "1"]
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.2", "1"]
            end
            port += 1
          end # end of looping all nvme disks
        end # end of checking for nvme disks
      end # end of virtualbox provider section
{% endif %}
{% if libvirt_provider %}
      # For details see: https://github.com/vagrant-libvirt/vagrant-libvirt
      srv.vm.provider "libvirt" do |libvirt, override|
        #libvirt.host = "localhost"
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        libvirt.watchdog :model => 'i6300esb', :action => 'reset'
	libvirt.storage_pool_path = kdevops_pool_path
{% if libvirt_session %}
	libvirt.qemu_use_session = true
	libvirt.uri = global_data['libvirt_cfg']['uri']
	libvirt.system_uri = global_data['libvirt_cfg']['system_uri']
	libvirt.socket = global_data['libvirt_cfg']['session_socket']
	libvirt.management_network_device = global_data['libvirt_cfg']['session_management_network_device']
{% else %}
	if nested
	  libvirt.management_network_name = 'vagrant-libvirt-private'
	  libvirt.management_network_address = '192.168.124.0/24'
	  libvirt.management_network_device = global_data['libvirt_cfg']['session_management_network_device']
	end
{% endif %}
        libvirt.memory = global_data["memory"]
        libvirt.cpus = global_data["cpus"]
        libvirt.emulator_path = global_data['libvirt_cfg']['emulator_path']
        if server_data["machine_type"]
          libvirt.machine_type = server_data["machine_type"]
        else
          if global_data['libvirt_cfg']['machine_type']
            libvirt.machine_type = global_data['libvirt_cfg']['machine_type']
          end
        end

{% if not libvirt_override_machine_type %}
        # Add an extra spare PCI-E root bus to be used for our NVME drives.
        #
        # Note that that up to at least recently we were not clashing with
        # the defaults brought up by libvirt-qemu, however recent versions
        # seem to start clashing up to pci.0,addr=0x3 so we'd see something
        # like:
        #
        # PCI: slot 3 function 0 not available for virtio-blk-pci, in use by nvme,id=(null) (Libvirt::Error)
        #
        # In fact, if you bumped the nvme drives to start at a custom
        # bus and addr say, pci.0,addr=0x8 you'd finally not run into clashes
        # up to  pci.0,addr=0x8. But we'd only be limited to 20 devices so
        # ZNS drives won't fit.
        #
        # So just create a PCI bridge dedicated for nvme drives. We use
        # 0x08 as we know this is avilable on modern x86-64 systems.
        # Eventually we may need to bump this to 0x9, but it would be better
        # instead to have vagant-libvit speak "add a new PCI express bridge"
        # and "add nvme drives".
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pci-bridge,id=custom-pci-for-nvme,chassis_nr=1,bus=pci.0,addr=0x8"
{% else %}
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "ioh3420,id=custom-pci-for-nvme,bus=pcie.0,addr=0x8,chassis=8"
{% if libvirt_enable_cxl %}
{% if libvirt_enable_cxl_demo_topo1 %}
        libvirt.qemuargs :value => "-machine"
        libvirt.qemuargs :value => "cxl=on"

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pxb-cxl,bus=pcie.0,id=cxl.0,bus_nr=52,addr=0x9"


        libvirt.qemuargs :value => "-object"
        libvirt.qemuargs :value => "memory-backend-file,id=kdevops-cxl-mem1,share=on,mem-path=/tmp/cxltest.raw,size=256M"

        libvirt.qemuargs :value => "-object"
        libvirt.qemuargs :value => "memory-backend-file,id=kdevops-cxl-lsa1,share=on,mem-path=/tmp/lsa.raw,size=256M"

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-rp,port=0,bus=cxl.0,id=kdevops_cxl_root_port1,chassis=0,slot=2"

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-type3,bus=kdevops_cxl_root_port1,memdev=kdevops-cxl-mem1,lsa=kdevops-cxl-lsa1,id=kdevops-cxl-pmem0"

        libvirt.qemuargs :value => "-M"
        libvirt.qemuargs :value => "cxl-fmw.0.targets.0=cxl.0,cxl-fmw.0.size=512M"
{% endif %}
{% endif %}
{% endif %}

        if server_data['pcipassthrough']
          server_data['pcipassthrough'].each do |key, value|
            libvirt.pci :domain => value['domain'], :bus => value['bus'], :slot => value['slot'], :function => value['function']
          end
        end
        if global_data['nvme_disks']
          port = 0
          port_count = global_data['nvme_disks'].size()
          global_data['nvme_disks'].each do |key, value|
            size = value['size']
            zoned = value['zoned']
            if zoned && !global_data['enable_zns']
		next
            end
	    if zoned
              logical_block_size = value['logical_block_size']
              physical_block_size = value['physical_block_size']
              zone_size = value['zone_size']
              zone_capacity = value['zone_capacity']
              zone_max_open = value['zone_max_open']
              zone_max_active = value['zone_max_active']
              zone_zasl = value['zone_zasl']
            end
            purpose = key
            port_plus = port + 1
	    key_id_prefix = global_data['libvirt_cfg']['nvme_disk_id_prefix']
	    disk_id = "#{key_id_prefix}#{port}"
	    serial_id = "serial#{port}"
            nvme_disk_postfix = global_data['libvirt_cfg']['nvme_disk_postfix']
            nvme_disk_name = "nvme#{port}n#{port_plus}.#{nvme_disk_postfix}"
            nvme_disk = nvme_path + "/" + "#{nvme_disk_name}"
            unless File.exist? (nvme_disk)
	      if provider == "libvirt"
	        cmd = "qemu-img create -f qcow2 #{nvme_disk} #{size}M"
	        ok = system(cmd)
	        if ! ok
                  puts "Command failed: #{cmd}"
                  exit
                end
              end
            end
{% if lacks_drive_virtio %}
            non_nvme_drive_interface = "ide"
{% else %}
            non_nvme_drive_interface = "virtio"
{% endif %}
{% if lacks_nvme_support %}
            libvirt.qemuargs :value => "-drive"
            libvirt.qemuargs :value => "file=#{nvme_disk},if=#{non_nvme_drive_interface}"
{% else %}
            # We're limited to 19 by default
            pci_function = 0 + port
            if zoned
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{nvme_disk},if=none,id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,id=nvme#{port},serial=#{serial_id},bus=custom-pci-for-nvme,addr=#{pci_function},zoned.zasl=#{zone_zasl}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme-ns,drive=#{disk_id},bus=nvme#{port},nsid=1,logical_block_size=#{logical_block_size},physical_block_size=#{physical_block_size},zoned=true,zoned.zone_size=#{zone_size},zoned.zone_capacity=#{zone_capacity},zoned.max_open=#{zone_max_open},zoned.max_active=#{zone_max_active}"
            else
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{nvme_disk},if=none,id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,drive=#{disk_id},serial=#{serial_id},bus=custom-pci-for-nvme,addr=#{pci_function}"
            end
            port += 1
{% endif %}
	  end
	  if provider == "libvirt"
	    cmd = "chgrp -R #{qemu_group} #{nvme_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	    cmd = "chmod -R g+rw #{nvme_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	  end # end of provider check for libvirt
	end # end of check for nvme disks for libvirt
      end # end of libvirt provider code
{% endif %}
    end # end of srv defined loop
  end # end of vagrant_boxes loop
end
