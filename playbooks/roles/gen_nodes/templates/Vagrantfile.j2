# -*- mode: ruby -*-
# # vi: set ft=ruby :
# Automatically generated file by kdevops {{ kdevops_version }}

# This file should only be edited to *grow* support for new features.
# This file is automatically created for you from a jinja2 template file
# and parsed with ansible after extra_vars.yaml file is created. The source is
# playbooks/roles/gen_nodes/templates/Vagrantfile.j2

Vagrant.require_version ">= 1.6.0"

require 'json'
require 'yaml'
require 'fileutils'
require 'rbconfig'

@os = RbConfig::CONFIG['host_os']

# XXX: upstream libvirt enhancement needed
#
# Vagrant allows multiple provider code to be supported easily, however
# this logic assuems all provider setup is supported through configuration
# variables from the provider. This is not the case for libvirt. We need
# to create the qemu image, and the libvirt vagrant provider doesn't have
# support to create the images as virtual box does. We run the commands
# natively, however this also reveals that on execution path even code
# for other providers gets executed regardless of the provider you are
# using *unless* that code is using provider specific variables. Best we
# can do for now is detect your OS and set a local variable where we *do*
# run different local code paths depending on the target provider.
#
# Right now we make these assumptions:
#
# Your OS            Provider
# -------------------------------
# Linux              libirt
# Mac OS X           virtualbox
provider = "libvirt"

# Are we in a nested virtualized environment? For now we just
# do something different if we're not on bare metal. We may want
# to do something different later to optimize this further but
# we have to draw the line somewhere.
nested = %x(which systemd-detect-virtd 1>/dev/null 2>&1 || systemd-detect-virt) != "none"

case
when @os.downcase.include?('linux')
  provider = "libvirt"
when @os.downcase.include?('darwin')
  provider = "virtualbox"
else
  puts "You OS hasn't been tested yet, go add support and send a patch."
  exit
end

# We assume if you are in kdevops/vagrant/ your project namespace is kdevops
project_namespace = File.basename(File.dirname(Dir.getwd))

node_config          =  "kdevops_nodes.yaml"
node_config_override =  "kdevops_nodes_override.yaml"

config_data = YAML.load_file(node_config)

override_data = false
if File.file?(node_config_override)
  override_data = true
  config_data_override = YAML.load_file(node_config_override)
end

global_data = config_data['vagrant_global']
vagrant_boxes = config_data['vagrant_boxes']

if override_data
  if config_data_override['vagrant_global']
    global_data = config_data_override['vagrant_global']
  end
  if config_data_override['vagrant_boxes']
    vagrant_boxes = config_data_override['vagrant_boxes']
  end
end

supported_provider = case provider
  when "virtualbox" then true
  when "libvirt" then true
  else false
end

if ! supported_provider
  puts "Unsupported provider: #{provider} on " + RbConfig::CONFIG['host_os']
  puts "Consider adding support and send a patch"
  exit
end

qemu_group = global_data['libvirt_cfg']['qemu_group']
qemu_group_auto = global_data['libvirt_cfg']['qemu_group_auto']

if qemu_group_auto
  if File.exist?('/etc/debian_version')
    qemu_group = 'libvirt-qemu'
  else
    qemu_group = 'qemu'
  end
end

kdevops_pool_path = global_data['storage_pool_path']

libvirt_session_public_network_dev = global_data['libvirt_cfg']['session_public_network_dev']

Vagrant.configure("2") do |config|
  vagrant_boxes.each do |server_data|
    # Using sync folders won't work for say openstack / aws / azure / gce, and
    # for that we'll use terraform, so best to allow whatever data we want to
    # sync be provisioned with ansible.
    config.vm.synced_folder './', '/vagrant', type: '9p', disabled: true, accessmode: "mapped", mount: false
    config.vm.define server_data["name"] do |srv|
      srv.vm.box = server_data['box'] ? server_data['box'] : global_data["box"]
      server_box_version = server_data['box_version'] ? server_data['box_version'] : global_data["box_version"]
      if server_box_version != ""
        srv.vm.box_version = server_box_version
      end
{% if not libvirt_session %}
      if ! nested
        srv.vm.network "private_network", ip: server_data["ip"]
      end
{% endif %}
      host_name = server_data["name"]
      node_custom_data_path = kdevops_pool_path + "/#{host_name}"
      FileUtils.mkdir_p(node_custom_data_path) unless File.exist?(node_custom_data_path)
      extra_disk_path = "#{node_custom_data_path}/" + "{{ extra_disk_path }}"
      FileUtils.mkdir_p(extra_disk_path) unless File.exist?(extra_disk_path)
{% if libvirt_enable_cxl %}
      cxl_path = "#{node_custom_data_path}" + "/cxl"
      FileUtils.mkdir_p(cxl_path) unless File.exist?(cxl_path)
{% endif %}
{% if virtualbox_provider %}
      srv.vm.provider "virtualbox" do |vb, override|
	if provider == "virtualbox" && ! global_data['virtualbox_cfg']['auto_update']
          # we'll need to run later: vagrant vbguest install
          config.vbguest.auto_update = false
        end
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        vb.memory = global_data["memory"]
        vb.cpus = global_data["cpus"]
        if global_data['extra_disks']
          port = 0
          port_count = global_data['extra_disks'].size()
          global_data['extra_disks'].each do |key, value|
            size = value['size']
	    # The "purpose" value below is currently an unused label at
	    # the moment. The serial number is we really use with the
	    # /dev/disk-id to ensure the extra drives are used as intended.
            purpose = key
            port_plus = port + 1
            extra_disk = extra_disk_path + "/{{ extra_disk_driver }}#{port}n#{port_plus}.{{ libvirt_extra_drive_format }}"

            # "Standard" provides a sparse file. That's what we want, we cheat
            # the OS and only use what we need. If you want the real file size
            # add a global config option and send a patch and justify it. I'd
            # like to hear about it. We use sparse files for libvirt as well
            # and should try to keep setup in sync.
            if (! File.file?(extra_disk))
              vb.customize ["createmedium", "disk", "--filename", extra_disk, "--variant", "Standard", "--format", "{{ libvirt_extra_drive_format | upper }}", "--size", size]
            end
            # Virtualbox supports only one nvme controller... this will fail
            # unless you are a Virtualbox hacker adding support for this :)
            if global_data['virtualbox_cfg']['nvme_controller_per_disk']
              # https://www.virtualbox.org/manual/ch08.html#vboxmanage-storagectl
              # This command attaches, modifies, and removes a storage
              # controller. After this, virtual media can be attached to the
              # controller with the storageattach command.
              extra_drive_name = "{{ extra_disk_driver }}#{port}"
              if (! File.file?(extra_disk))
                vb.customize ["storagectl", :id, "--name", "#{extra_drive_name}", "--add", "pcie", "--controller", "{{ extra_disk_controller }}", "--portcount", 1, "--bootable", "off"]
              end
	      # Now attach the drive
	      vb.customize ["storageattach", :id, "--storagectl", "#{extra_drive_name}", "--type", "hdd", "--medium", extra_disk, "--port", 0]
            else
              if (port == 0 && !File.file?(extra_disk))
                vb.customize ["storagectl", :id, "--name", "{{ extra_disk_driver }}0", "--add", "pcie", "--controller", "{{ extra_disk_controller }}", "--portcount", port + 1, "--bootable", "off"]
              end
	      vb.customize ["storageattach", :id, "--storagectl", "{{ extra_disk_driver }}0", "--type", "hdd", "--medium", extra_disk, "--port", port]
            end

            if global_data['enable_sse4']
	      # Support for the SSE4.x instruction is required in some versions of VB.
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.1", "1"]
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.2", "1"]
            end
            port += 1
          end # end of looping all extra disks
        end # end of checking for extra disks
      end # end of virtualbox provider section
{% endif %}
{% if libvirt_provider %}
      # For details see: https://github.com/vagrant-libvirt/vagrant-libvirt
      srv.vm.provider "libvirt" do |libvirt, override|
        #libvirt.host = "localhost"
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        libvirt.watchdog :model => 'i6300esb', :action => 'reset'
        libvirt.storage_pool_path = kdevops_pool_path
{% if libvirt_host_passthrough %}
        libvirt.cpu_mode = 'host-passthrough'
{% endif %}
{% if libvirt_storage_pool_create %}
	libvirt.storage_pool_name = '{{ libvirt_storage_pool_name }}'
{% endif %}
{% if libvirt_session %}
	libvirt.qemu_use_session = true
	libvirt.uri = global_data['libvirt_cfg']['uri']
	libvirt.system_uri = global_data['libvirt_cfg']['system_uri']
	libvirt.socket = global_data['libvirt_cfg']['session_socket']
	libvirt.management_network_device = global_data['libvirt_cfg']['session_management_network_device']
{% else %}
	if nested
	  libvirt.management_network_name = 'vagrant-libvirt-private'
	  libvirt.management_network_address = '192.168.124.0/24'
	  libvirt.management_network_device = global_data['libvirt_cfg']['session_management_network_device']
	end
{% endif %}
        libvirt.memory = global_data["memory"]
        libvirt.cpus = global_data["cpus"]
        libvirt.emulator_path = global_data['libvirt_cfg']['emulator_path']
        if server_data["machine_type"]
          libvirt.machine_type = server_data["machine_type"]
        else
          if global_data['libvirt_cfg']['machine_type']
            libvirt.machine_type = global_data['libvirt_cfg']['machine_type']
          end
        end
        # Add an extra spare PCI or PCI-E root bus to be used for extra drives.
        #
        # We use a dedicated PCI or PCI-E root bus to not clash with defaults
        # which libvirt may use on PCI root bus (pci.0 if PCI, pcie.0 if PCI-E).
        # For a while in kdevops we were not clashing with the defaults brought
        # up by libvirt-qemu, however recent versions seem to start clashing up
        # to pci.0,addr=0x3 so we'd see an error like:
        #
        # PCI: slot 3 function 0 not available for virtio-blk-pci, in use by nvme,id=(null) (Libvirt::Error)
        #
        # Just create a PCI or PCI-E root bus dedicated for extra drives. We
        # use 0x08 to place this PCI / PCI-E root bus as we know this is
        # avilable on modern x86-64 systems. Eventually we may need to bump
        # this to 0x9, but it would be better instead to have vagant-libvirt
        # speak "add a new PCI or PCI-E root bus" and "add extra drives" whether
        # that is nvme or virtio.
        #
{% if not libvirt_override_machine_type %}
        # For i440x on x86_64 (default on libvirt as of today) we use PCI, not
        # PCI-E. Below assumes i440x. i440x cannot support CXL as it does not
        # suport PCI-E.
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pci-bridge,id=custom-pci-for-{{ extra_disk_driver }},chassis_nr=1,bus=pci.0,addr=0x8"
{% else %}
{% if libvirt_machine_type == "q35" %}
        libvirt.qemuargs :value => "-global"
        libvirt.qemuargs :value => "ICH9-LPC.disable_s3=0"
        libvirt.qemuargs :value => "-global"
        libvirt.qemuargs :value => "ICH9-LPC.disable_s4=0"
{% endif %}
	# This is for exclusive PCIe hierarchies.
	#
        # For 'q35' on x86_64 and 'virt' for AArch64 we use can use PCI-E
        # only hierarchies.
        #
	# We add a dedicated PCI root bus (pxb-pcie) just for dedicated PCI
	# root ports for extra drives, this will be pcie.1.
	#
        # We can keep doing this to support new technologies. For instance,
        # it may be reasonable to add a PCI-E root bus also to support PCI-E
        # hotplug on PCI-E capable systems and avoid clashes with libvirt
	# defaults on pcie.0. CXL uses its own host bridge so device
	# enumeration is pretty well isolated already so no need to do anything
	# extra there.
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pxb-pcie,id=pcie.1,bus_nr=32,bus=pcie.0,addr=0x8"
{% if libvirt_enable_cxl %}
        libvirt.qemuargs :value => "-machine"
        libvirt.qemuargs :value => "cxl=on"
	cpus = global_data["cpus"]
	memory = global_data["memory"]
	libvirt.numa_nodes = [
		{:cpus => "0-#{cpus - 1}", :memory => global_data["memory"]}
	]
	libvirt.qemuargs :value => "-m"
	libvirt.qemuargs :value => "maxmem=#{memory *2}M,slots=16"
	# CXL host bus
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pxb-cxl,bus=pcie.0,id=cxl.0,bus_nr=52,addr=0x9"


        cxl_mem1_path = cxl_path + "/cxl-mem1.raw"
        libvirt.qemuargs :value => "-object"
        libvirt.qemuargs :value => "memory-backend-file,id=kdevops-cxl-mem1,share=on,mem-path=#{cxl_mem1_path},size=4G"

	# Label Storage Area, used to store CXL namespace labels and region labels
        cxl_lsa1_path = cxl_path + "/cxl-lsa1.raw"
        libvirt.qemuargs :value => "-object"
        libvirt.qemuargs :value => "memory-backend-file,id=kdevops-cxl-lsa1,share=on,mem-path=#{cxl_lsa1_path},size=256M"

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-rp,port=0,bus=cxl.0,id=kdevops_cxl_root_port0,chassis=0,slot=2"

{% if libvirt_enable_cxl_demo_topo2 %}
	libvirt.qemuargs :value => "-device"
	libvirt.qemuargs :value => "cxl-rp,port=1,bus=cxl.0,id=kdevops_cxl_root_port1,chassis=0,slot=3"
{% endif %} # TOPO 2


{% if libvirt_enable_cxl_switch_topo1 %}
	libvirt.qemuargs :value => "-device"
	libvirt.qemuargs :value => "cxl-upstream,bus=kdevops_cxl_root_port0,id=kdevops_cxl_us0,addr=0.0,multifunction=on"
	libvirt.qemuargs :value => "-device"
	libvirt.qemuargs :value => "cxl-switch-mailbox-cci,bus=kdevops_cxl_root_port0,addr=0.1,target=kdevops_cxl_us0"
	libvirt.qemuargs :value => "-device"
	libvirt.qemuargs :value => "cxl-downstream,port=0,bus=kdevops_cxl_us0,id=kdevops_cxl_swport0,chassis=0,slot=4"
	libvirt.qemuargs :value => "-device"
	libvirt.qemuargs :value => "cxl-downstream,port=1,bus=kdevops_cxl_us0,id=kdevops_cxl_swport1,chassis=0,slot=5"
	libvirt.qemuargs :value => "-device"
	libvirt.qemuargs :value => "cxl-type3,bus=kdevops_cxl_swport0,memdev=kdevops-cxl-mem1,lsa=kdevops-cxl-lsa1,id=kdevops-cxl-pmem0"
{% else %} # !switch topo1
	{% if not libvirt_enable_cxl_dcd_topo1 %}
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-type3,bus=kdevops_cxl_root_port0,memdev=kdevops-cxl-mem1,lsa=kdevops-cxl-lsa1,id=kdevops-cxl-pmem0"
	{% else %} # dcd_topo1
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-type3,bus=kdevops_cxl_root_port0,nonvolatile-dc-memdev=kdevops-cxl-mem1,lsa=kdevops-cxl-lsa1,id=kdevops-cxl-dcd0,num-dc-regions=2"
	{% endif %} # dcd topo1
{% endif %} # !switch topo1
        libvirt.qemuargs :value => "-M"
        libvirt.qemuargs :value => "cxl-fmw.0.targets.0=cxl.0,cxl-fmw.0.size=8G"
{% if libvirt_enable_qmp %}
        libvirt.qemuargs :value => "-qmp"
{% if libvirt_qemu_qmp_wait %}
        libvirt.qemuargs :value => "{{ libvirt_qemu_qmp_string }},wait=on"
{% else %} # !qmp_wait
        libvirt.qemuargs :value => "{{ libvirt_qemu_qmp_string }},wait=off"
{% endif %} # libvirt_qemu_qmp_wait
{% endif %} # libvirt_enable_qmp
{% endif %} # CXL
{% endif %}

        if server_data['pcipassthrough']
          server_data['pcipassthrough'].each do |key, value|
            libvirt.pci :domain => value['domain'], :bus => value['bus'], :slot => value['slot'], :function => value['function']
          end
        end
        if global_data['extra_disks']
          port = 0
          port_count = global_data['extra_disks'].size()
          global_data['extra_disks'].each do |key, value|
            size = value['size']
            zoned = value['zoned']
            largio = value['largio']
            if zoned && !global_data['enable_zns']
		next
            end
            extra_drive_largio_args=""
	    if zoned
              logical_block_size = value['logical_block_size']
              physical_block_size = value['physical_block_size']
              zone_size = value['zone_size']
              zone_capacity = value['zone_capacity']
              zone_max_open = value['zone_max_open']
              zone_max_active = value['zone_max_active']
              zone_zasl = value['zone_zasl']
            end
	    if largio
              largeio_logical_block_size = value['logical_block_size']
              largeio_physical_block_size = value['physical_block_size']
              extra_drive_logical_block_size_args = "logical_block_size=#{largeio_logical_block_size}"
              extra_drive_physical_block_size_args = "physical_block_size=#{largeio_physical_block_size}"
              extra_drive_largio_args=",#{extra_drive_logical_block_size_args},#{extra_drive_physical_block_size_args}"
            end
            purpose = key
            port_plus = port + 1
	    key_id_prefix = "{{ libvirt_extra_drive_id_prefix }}"
	    disk_id = "#{key_id_prefix}#{port}"
	    serial_id = "kdevops#{port}"
            extra_disk_name = "{{ extra_disk_driver }}#{port}n#{port_plus}.{{ libvirt_extra_drive_format }}"
            extra_disk = extra_disk_path + "/" + "#{extra_disk_name}"
            unless File.exist? (extra_disk)
	      if provider == "libvirt"
	        cmd = "qemu-img create -f {{ libvirt_extra_drive_format }}  #{extra_disk} #{size}M"
	        ok = system(cmd)
	        if ! ok
                  puts "Command failed: #{cmd}"
                  exit
                end
              end
            end
{% if not libvirt_override_machine_type %}
            bus_for_extra_drives = "custom-pci-for-{{ extra_disk_driver }}"
            pci_function = 0 + port
{% else %}
	    # A PCI-E root port must be added per extra drive.
            # Chassis must be something unique for the entire topology it seems (?)
            chassis = 50 + port
            bus_for_extra_drives = "pcie-port-for-{{ extra_disk_driver }}-#{port}"
            libvirt.qemuargs :value => "-device"
            libvirt.qemuargs :value => "pcie-root-port,id=#{bus_for_extra_drives},multifunction=on,bus=pcie.1,addr=0x#{port},chassis=#{chassis}"
            pci_function = "0x0"
{% endif %}
{% if libvirt_extra_storage_drive_ide %}
            libvirt.qemuargs :value => "-drive"
            libvirt.qemuargs :value => "file=#{extra_disk},format={{ libvirt_extra_drive_format }},aio={{ libvirt_extra_storage_aio_mode }},cache={{ libvirt_extra_storage_aio_cache_mode }},if=ide,serial=#{serial_id}"
{% elif libvirt_extra_storage_drive_virtio %}
            virtio_pbs = "{{ libvirt_extra_storage_virtio_physical_block_size }}"
            virtio_lbs = "{{ libvirt_extra_storage_virtio_logical_block_size }}"
            libvirt.qemuargs :value => "-object"
            libvirt.qemuargs :value => "iothread,id=kdevops-virtio-iothread-#{port}"
            libvirt.qemuargs :value => "-drive"
            libvirt.qemuargs :value => "file=#{extra_disk},format={{ libvirt_extra_drive_format }},if=none,aio={{ libvirt_extra_storage_aio_mode }},cache={{ libvirt_extra_storage_aio_cache_mode }},id=#{disk_id}"
            libvirt.qemuargs :value => "-device"
            libvirt.qemuargs :value => "virtio-blk-pci,scsi=off,drive=#{disk_id},id=virtio-#{disk_id},serial=#{serial_id},bus=#{bus_for_extra_drives},addr=#{pci_function},iothread=kdevops-virtio-iothread-#{port}#{extra_drive_largio_args},logical_block_size=#{virtio_lbs},physical_block_size=#{virtio_pbs}"
{% elif libvirt_extra_storage_drive_nvme  %}
            if ! largio
              nvme_lbs = "{{ libvirt_extra_storage_nvme_logical_block_size }}"
              extra_drive_largio_args=",logical_block_size=#{nvme_lbs},physical_block_size=#{nvme_lbs}"
            end
            extra_drive_interface = "none"
            if zoned
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{extra_disk},format={{ libvirt_extra_drive_format }},aio={{ libvirt_extra_storage_aio_mode }},cache={{ libvirt_extra_storage_aio_cache_mode }},if=#{extra_drive_interface},id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,id={{ extra_disk_driver }}#{port},serial=#{serial_id},bus=#{bus_for_extra_drives},addr=#{pci_function},zoned.zasl=#{zone_zasl}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme-ns,drive=#{disk_id},bus={{ extra_disk_driver }}#{port},nsid=1,logical_block_size=#{logical_block_size},physical_block_size=#{physical_block_size},zoned=true,zoned.zone_size=#{zone_size},zoned.zone_capacity=#{zone_capacity},zoned.max_open=#{zone_max_open},zoned.max_active=#{zone_max_active}"
            else
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{extra_disk},format={{ libvirt_extra_drive_format }},aio={{ libvirt_extra_storage_aio_mode }},cache={{ libvirt_extra_storage_aio_cache_mode }},if=#{extra_drive_interface},id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,id={{ extra_disk_driver }}#{port},serial=#{serial_id},bus=#{bus_for_extra_drives},addr=#{pci_function}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme-ns,drive=#{disk_id},bus={{ extra_disk_driver }}#{port},nsid=1#{extra_drive_largio_args}"
            end
{% endif %}
            port += 1
	  end
	  if provider == "libvirt"
	    cmd = "sudo chgrp -R #{qemu_group} #{node_custom_data_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	    cmd = "sudo chmod -R g+rw #{node_custom_data_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	  end # end of provider check for libvirt
	end # end of check for extra disks for libvirt
{% if bootlinux_9p %}

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "{{ bootlinux_9p_driver }},fsdev={{ bootlinux_9p_fsdev }},mount_tag={{ bootlinux_9p_mount_tag }},bus=pcie.0,addr=0x10"
        libvirt.qemuargs :value => "-fsdev"
        libvirt.qemuargs :value => "local,id={{ bootlinux_9p_fsdev }},path={{ bootlinux_9p_host_path }},security_model={{ bootlinux_9p_security_model }}"

{% endif %}
      end # end of libvirt provider code
{% endif %}
    end # end of srv defined loop
  end # end of vagrant_boxes loop
end
