if KDEVOPS_WORKFLOW_ENABLE_FSTESTS

config HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG
	bool
	default n

config HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL
	bool
	default n

config HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_RESET
	bool
	default n

# This nameless bool is done so that when kconfig gets support to output a yaml
# file we can simply remove the respective makefile entry which queries for this
# as kconfig would have done it for us.
#
# When we do add support to kconfig to output extra_vars.yaml we will also
# add support so that only kconfig symbols which have an associated new option,
# say, select_yaml_output, will it be used to also be output to the yaml file.
# That would ensure that our extra_vars.yaml file won't be as long as our
# existing .config files.
#
# XXX: if other filesystems have other similar bools we should do something
# similar for them to make it easier later.
config FSTESTS_TMPFS_ENABLE
	bool

config FSTESTS_NFS_ENABLE
	bool

config FSTESTS_USES_NO_DEVICES
	bool

config FSTESTS_GENERATE_SIMPLE_CONFIG_ENABLE
	bool

config FSTESTS_GENERATE_NVME_LIVE_CONFIG_ENABLE
	bool

choice
	prompt "Target filesystem to test"
	default FSTESTS_BTRFS

config FSTESTS_XFS
	bool "xfs"
	help
	  This will target testing the xfs filesystem.

config FSTESTS_BTRFS
	bool "btrfs"
	help
	  This will target testing the btrfs filesystem.

config FSTESTS_EXT4
	bool "ext4"
	help
	  This will target testing the ext4 filesystem.

config FSTESTS_NFS
	bool "nfs"
	select FSTESTS_NFS_ENABLE
	help
	  This will target testing the Network File System (NFS).

config FSTESTS_TMPFS
	bool "tmpfs"
	select FSTESTS_TMPFS_ENABLE
	select FSTESTS_USES_NO_DEVICES
	select FSTESTS_GENERATE_SIMPLE_CONFIG_ENABLE
	help
	  This will target testing the tmpfs (Virtual Memory File System).

endchoice

config FSTESTS_FSTYP
	string
	default "xfs" if FSTESTS_XFS
	default "btrfs" if FSTESTS_BTRFS
	default "ext4" if FSTESTS_EXT4
	default "nfs" if FSTESTS_NFS
	default "tmpfs" if FSTESTS_TMPFS
	help
	  The filesystem to test. This will be automatically set depending on
	  the above choice, you however can override this manually.

config FSTESTS_TFB_COPY_ENABLE
	bool "Enable trimming extremely large output files at copy time"
	default y
	help
	  Some tests may have test bugs leading to insane test *.out.bad file
	  sizes. An example is generic/251 produces ~14 GiB of 251.out.bad on
	  the test section xfs_rtlogdev. Although these cases are pathological
	  and rare kdevops supports copying over test results for failed files
	  and 'make fstests-baseline-results' will fail for the nodes where
	  those files are in running out of memory for the task. Enable this
	  to trim those files based on a max file size.

	  Disable this option if are enabling kdevops kernel-ci and you want to
	  keep insanely large files. kdevops will only trim large files with
	  the 'make fstests-baseline-results' command, so if you just run
	  'make fstests-baseline' kdevops will not trim these files for you.
	  Since enabling kernel-ci uses the 'make fstests-baseline-results'
	  kernel-ci would automatically trim large files if this is enabled.
	  If you don't use the kdevops kernel-ci and are just running baselines
	  manually, ie, running 'make fstests-baseline' trimming will not happen
	  unless you manually run 'make fstests-baseline-results'. This allows
	  you still inspect insanely large files even if this feature is enabled
	  if running baselines manually.

if FSTESTS_TFB_ENABLE

config FSTESTS_TFB_COPY_LIMIT
	int "Max size of test results files allowed prior to copy"
	default 10485760
	help
	  Trim files larger than this size in bytes with a simple TFB text on
	  them. By default we use 10 MiB limit.

endif # FSTESTS_TFB_ENABLE

config FSTESTS_WATCHDOG
	bool "Enable kdevops fstests watchdog"
	default y if HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG
	default n if !HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG
	depends on KERNEL_CI
	help
	  Enable the fstests watchdog which lets you keep tabs on progress
	  of fstests on different spawned systems.

	  Enabling this option will allow the kdevops fstests_watchdog.py to
	  run and log into the workflows/fstests/watchdog/$FSTYP.txt the status
	  of all known tests.

	  It is safe to enable as no action is taken by default, it is just
	  informational. If you however don't even want this information logged
	  you can disable this.

if FSTESTS_WATCHDOG

config FSTESTS_WATCHDOG_CHECK_TIME
	int "How often to run the festests_watchdog in seconds"
	default 5
	help
	  How often to trigger running the fstests watchdog, in seconds.

config FSTESTS_WATCHDOG_MAX_NEW_TEST_TIME
	int "Max minutes to wait for a new test before assuming a hang"
	default 60
	help
	  Max value of time in minutes a new test should take before assuming
	  the test is hung.

	  Once a test has completed or failed fstests captures the amount of
	  seconds it took to run that test on a file called check.time. This
	  file consists of two columns, the first with the test name and the
	  second column with the amount of time in seconds it took to run a
	  test. When we run a test for the first time there is no known prior
	  last test time, as there would be no check.time files present from a
	  prior run.

	  The amount of time it takes to run a test will also depend highly on
	  the system, drive and filesystem you are running the test on and so
	  check.time values are completely system specific. However, we can
	  often run into a test which is hung but fstests seems to keep running,
	  without providing feedback about a possibly hung test. In order to
	  tell if a test is hung we must define a value for max time any new test
	  can take. This value is system specific and so must be defined by the
	  person configuring the test, however sensible default values are
	  defined.

	  The kdevops fstests_watchdog.py uses this max value to know when a
	  test is on a system is hung for a test's first run. If a test had
	  already completed before, and so had an entry for the test on the
	  last test's check.time file for the target system, other heuristics
	  are used.

	  Setting this value is only informational, so that fstests_watchdog.py
	  can print a status of "hung" suspect when it finds a test goes over
	  this value on a first run.

config FSTESTS_WATCHDOG_HUNG_MULTIPLIER_LONG_TESTS
	int "Multiplier for amount of time before assuming a test is hung"
	default 10
	help
	  If we know a test took 1 minute the last time we ran it, setting the
	  multiplier to 10 will let the watchdog decide a test is hung if 10
	  minutes have passed and the test has not completed yet.

	  This multiplier is only used for tests for which we have a prior
	  last run time and that run time is greater than 30 seconds.

config FSTESTS_WATCHDOG_HUNG_FAST_TEST_MAX_TIME
	int "Max time in minutes to wait before declaring quick tests as hung"
	default 5
	help
	  If we know a test took between 1 second and 30 seconds to run in a
	  prior test we can safely assign a timeout value in minutes for these
	  quick tests for which we will let run before allowing the fstests
	  watchdog to assume the test is hung.

config FSTESTS_WATCHDOG_KILL_TASKS_ON_HANG
	bool "Enable killing local tasks on hang detection"
	default y if HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL
	default n if !HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL
	help
	  Enable the fstest watchdog to kill local controller tasks and if
	  enabled send a report if a hang / timeout occurs but leave the
	  systems which are hung / timed out alive for further inspection.

config FSTESTS_WATCHDOG_RESET_HUNG_SYSTEMS
	bool "Reset systems when the watchdog detects they are hung"
	default y if HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_RESET
	default n if !HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_RESET
	depends on VAGRANT
	depends on FSTESTS_WATCHDOG_KILL_TASKS_ON_HANG
	help
	  If this option is enabled, kdevops will reset all systems after a
	  hang is detected. This is a bad idea unless you know what you are
	  doing, ie, if this is option is enabled for you automatically.

	  The tests which caused the hangs found will be logged. Enabling
	  this is only desirable if you don't care to manually inspect
	  hosed systems when a hang happens. This may be useful in the
	  future if we enable automatic tasks which create a baseline for
	  you, for instance, but for this to be more useful, we'd likely
	  have to also implement a way to try to capture kernel logs
	  before a reset.

	  We currently only enable this for vagrant provisioning, given we have
	  to figure out a unified way to express a reset for cloud solution and
	  also figure out a way to enable this for bare metal. For vagrant we
	  currently use 'virsh reset' and so this works only if on libvirt.

endif # FSTESTS_WATCHDOG

if FSTESTS_XFS

menu "Configure how to test XFS"
source "workflows/fstests/xfs/Kconfig"
endmenu

endif # FSTESTS_XFS

if FSTESTS_BTRFS

menu "Configure how to test btrfs"
source "workflows/fstests/btrfs/Kconfig"
endmenu

endif # FSTESTS_BTRFS

if FSTESTS_EXT4

menu "Configure how to test ext4"
source "workflows/fstests/ext4/Kconfig"
endmenu

endif # FSTESTS_EXT4

if FSTESTS_NFS

menu "Configure how nfs should be tested"
source "workflows/fstests/nfs/Kconfig"
endmenu

endif # FSTESTS_NFS

if FSTESTS_TMPFS

menu "Configure how tmpfs should be tested"
source "workflows/fstests/tmpfs/Kconfig"
endmenu

endif # FSTESTS_TMPFS

config FSTESTS_GIT
	string "The fstests git tree to clone"
	default "https://git.kernel.org/pub/scm/fs/xfs/xfstests-dev.git" if !GIT_ALTERNATIVES
	default "https://github.com/linux-kdevops/fstests.git" if GIT_LINUX_KDEVOPS_GITHUB
	default "https://gitlab.com/linux-kdevops/fstests.git" if GIT_LINUX_KDEVOPS_GITLAB
	default "https://github.com/btrfs/fstests.git" if FSTESTS_BTRFS
	help
	  The fstests git tree to clone.

config FSTESTS_DATA
	string "Where to clone the fstests git tree to"
	default "{{data_path}}/fstests"
	help
	  This is the target location of where to clone the above git tree.
	  Note that {{data_path}} corresponds to the location set by the
	  configuration option CONFIG_WORKFLOW_DATA_PATH.

config FSTESTS_GIT_VERSION
	string "The fstests git tree version(branch/tag/commit)"
	default "HEAD"
	help
	  The fstests git tree version.

config FSTESTS_DATA_TARGET
	string "The target fstests install directory"
	default "/var/lib/xfstests"
	help
	  The directory where fstests will be installed. Modifying this probably
	  won't work well yet.

config FSTESTS_TESTDEV_UNDER_DEVELOPMENT
	bool "Enable test drive options still under development"
	help
	  You will want to enable this if you know what you are doing and
	  want to help develop and test new features which let you use
	  real drives to test fstests with.

if !FSTESTS_TMPFS && !FSTESTS_NFS

choice
	prompt "Strategy for device pool"
	default FSTESTS_TESTDEV_SPARSEFILE_GENERATION

config FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	bool "Create sparse files for test devices"
	select FSTESTS_GENERATE_SIMPLE_CONFIG_ENABLE
	help
	  When testing a filesystem with fstests you must provide the
	  test devices to be used to test. By selecting this option
	  sparse files will be created for you on a target path, and
	  loopback devices will be set up for you so you can use these
	  loopback devices to test the filesystem.

	  This is useful if you don't really want to test creating a filesystem
	  against real block devices or are constrained / limited by the amount
	  of block devices you have. By using sparse files you are also
	  reducing the amount of hard drive space you actually need to only the
	  amount of actual space used by all the tests. This varies by the
	  filesystem you are testing.

	  We'll create 12 sparse files for you, and so you can run tests which
	  require up to 12 block devices.

	  kdevops started using the creation of loop drives starting at
	  /dev/loop5 instead of /dev/loop0 since its first commit of
	  gendisks.sh [0]. At least on v3.0 kernel the loopback driver would
	  would create automatically 8 loopback drives if no max loop was
	  specified. The practice of starting to use loop5 all the way up to
	  /dev/loop16 likely started to avoid conflicts with existing old
	  distribution uses of the first 5 loop drives, and to provide a
	  consistent way to ensure loop drives are always the same. It just
	  so happens that if need many partitions on a real drive you create
	  one extended partition and many logical drives, in that setup you
	  also start at /dev/<device>5. We upkeep this practice simply for
	  historic reasons.

	  The first main test device TEST_DEV is set to /dev/loop16.
	  If a LOGWRITES_DEV is used this /dev/loop15 is used and is used for
	  power failure tests.

	  TEST_LOGDEV is only used when a filesystem supports an external
	  journal, this is uses the same /dev/loop15 since we currently disable
	  LOGWRITES_DEV when testing TEST_LOGDEV. This is just because we ran
	  out of loop back drives.

	  For XFS with a realtime device TEST_RTDEV uses /dev/loop13 and
	  SCRATCH_RTDEV set set to /dev/loop14.

	  The SCRATCH_DEV_POOL is set to these devices:

	   /dev/loop5   /dev/loop6   /dev/loop7   /dev/loop8
	   /dev/loop9   /dev/loop10  /dev/loop11  /dev/loop12

	  [0] https://github.com/mcgrof/oscheck/commit/218e4a4fce6add0b2b059fb6bf93e1a768e4948d

config FSTESTS_TESTDEV_NVME_PARTITION_EUIS
	bool "Use first real NVMe drive and create partitions"
	select EXTRA_STORAGE_SUPPORTS_512
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	select FSTESTS_GENERATE_NVME_LIVE_CONFIG_ENABLE
	help
	  If you want to test using real storage drives the only way
	  to reliably ensure they are the same drives upon a reboot is
	  to rely on the /dev/disk/by-id. This strategy will find the
	  first NVMe drives found on /dev/disk/by-id/nvme.eui.* and it
	  to create partitions required to test a filesystem on.

	  Note that sparse file generation testing / loop back drive test
	  (FSTESTS_TESTDEV_SPARSEFILE_GENERATION) relies on 12 drives, it starts
	  its testing on TEST_DEV on /dev/loop16 and goes back, the loop drive
	  count starts from /dev/loop16 all the way to /dev/loop5 to mimic what
	  you would get if you had to create an extended partition and then
	  create a set of extra logical partitions to test. And so this will
	  remove all partitions on that drive and then create an extended
	  partition, followed by 11 logical partitions:

	  /dev/disk/by-id/nvme.eui.*-part5 - /dev/disk/by-id/nvme.eui.*-part16

	  These are symlinks which map to their respective on boot:

	  /dev/nvme*p5 - /dev/nvme*p16

	  Note that the real NVMe drives could be passthrough onto a guest.
	  This has not been tested without that and so typically qemu nvme
	  drives are used for the other things.

	  TEST_DEV      uses /dev/*p16

	  When an XFS realtime subvolume is used:

	  SCRATCH_RTDEV uses /dev/*p14
	  TEST_RTDEV    uses /dev/*p13

	  Both of these are mutually exclusive for now:

	  TEST_LOGDEV   uses /dev/*p15
	  LOGWRITES_DEV uses /dev/*p15

	  The SCRATCH_DEV_POOL is set to these devices:

	   /dev/*part5   /dev/*part6   /dev/*part7   /dev/*part8
	   /dev/*part9   /dev/*part10  /dev/*part11  /dev/*part12

	  With this strategy there are no unused partitions.

config FSTESTS_TESTDEV_NVME_EUIS
	bool "Use at least 7 real NVMe drives detected by EUI"
	select EXTRA_STORAGE_SUPPORTS_512
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	select FSTESTS_GENERATE_NVME_LIVE_CONFIG_ENABLE
	help
	  If you want to test using real storage drives the only way
	  to reliably ensure they are the same drives upon a reboot is
	  to rely on the /dev/disk/by-id. This strategy will find the
	  first 7 NVMe drives found on /dev/disk/by-id/nvme.eui.* and use
	  them to set up the drives for testing.

	  Note that sparse file generation testing / loop back drive test
	  (FSTESTS_TESTDEV_SPARSEFILE_GENERATION) relies on 11 drives, that's
	  quite a bit so for now we lower the threshold to 11 drives. If you
	  don't have this many drives you can instead use the next option
	  which let's you create partitions for you.

	  Note that the real NVMe drives could be passthrough onto a guest.
	  This has not been tested without that and so typically qemu nvme
	  drives are used for the other things.

	  We'll want to populate:
	  - TEST_DEV
	  - SCRATCH_DEV_POOL we'll use 5 drives for this

	  Due to lack of drives you won't be able to test using:

	  - LOGWRITES_DEV
	  - SCRATCH_RTDEV

endchoice

endif # FSTESTS_TMPFS

config FSTESTS_TESTDEV_NVME_FALLBACK_MODEL_SERIAL
	bool "Fallback to using NVMe model an serial"
	depends on FSTESTS_TESTDEV_NVME_PARTITION_EUIS || FSTESTS_TESTDEV_NVME_EUIS
	default y
	help
	  As per the NVMe base specification 2.0c under section 3.2.1.6 NSID and
	  Namespace Usage, devices which support namespaces, ANA reporting or
	  NVM sets need to pick one of NGUID, EUI64 or UUID to implement and
	  suport. So EUIs are not required and not all drives support all these
	  features. So some drives may exist which don't use EUIs. To help with
	  this this allows you to use a fallback of the Linux kernel symlink
	  fallback of:

	    /dev/disk/by-id/nvme-$(model)-$(serial)

	  This mechanism will by default skip qemu NVMe drives.

	  Disable this if you want to only use drives which support EUI symlinks.

if FSTESTS_TESTDEV_SPARSEFILE_GENERATION

config FSTESTS_SPARSE_DEV
	string "Device to use to create a filesystem on for sparse files"
	default "/dev/disk/by-id/nvme-QEMU_NVMe_Ctrl_kdevops1" if VAGRANT && LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	default "/dev/disk/by-id/virtio-kdevops1" if VAGRANT && LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	default "/dev/disk/by-id/ide-kdevops1" if VAGRANT && LIBVIRT_EXTRA_STORAGE_DRIVE_IDE
	default "/dev/nvme2n1" if TERRAFORM_AWS_INSTANCE_M5AD_4XLARGE
	default "/dev/nvme1n1" if TERRAFORM_GCE
	default "/dev/sdd" if TERRAFORM_AZURE
	default TERRAFORM_OCI_SPARSE_VOLUME_DEVICE_FILE_NAME if TERRAFORM_OCI
	help
	  The device to use to create a filesystem on where we will place
	  sparse files.

choice
	prompt "Filesystem to use where sparse files will live"
	default FSTESTS_SPARSE_BTRFS

config FSTESTS_SPARSE_XFS
	bool "xfs"
	help
	  This will target testing the xfs filesystem.

config FSTESTS_SPARSE_BTRFS
	bool "btrfs"
	help
	  This will target testing the btrfs filesystem.

config FSTESTS_SPARSE_EXT4
	bool "ext4"
	help
	  This will target testing the ext4 filesystem.

endchoice

config FSTESTS_SPARSE_FSTYPE
	string
	default "xfs" if FSTESTS_SPARSE_XFS
	default "btrfs" if FSTESTS_SPARSE_BTRFS
	default "ext4" if FSTESTS_SPARSE_EXT4
	help
	  The filesystem to use where we will place the generated sparse files.
	  This is not the filesystem we will test, this is just where the sparse
	  files used will live.

config FSTESTS_SPARSE_LABEL
	string "Filesystem label to use for the filesystem where sparsefiles live"
	default "sparsefiles"
	help
	  The filesystem label to create and later use to mount the partition on
	  the /etc/fstab file to which represents where sparsefiles live.

config FSTESTS_SPARSE_FILE_PATH
	string "Path to directory where to generate sparse files"
	default "/media/sparsefiles"
	help
	  The directory where to create sparse files to be used to represent
	  actual devices used.

config FSTESTS_SPARSE_FILE_SIZE
	string "Size of the sparse files to use per target device"
	default "20G"
	help
	  The size of the sparse files to create, this is the -s argument passed
	  to the command truncate.

config FSTESTS_SPARSE_FILENAME_PREFIX
	string "The sparse file name prefix to use"
	default "sparse-disk"
	help
	  This is the prefix for the name of the sparse file to create.
	  For instance if FSTESTS_SPARSE_FILE_PATH is set to /media/truncated
	  and you set this prefix to sparse-disk, the first sparse file to create
	  will be named /media/truncated/sparse-disk5. The name used is simply for
	  pure convenience to the developer and to make it clear what type of
	  file backing is provided for the loopback device.

endif # FSTESTS_TESTDEV_SPARSEFILE_GENERATION

config FSTESTS_TEST_DEV
	string "The device to use TEST_DEV on fstests"
	default "/dev/loop16" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	help
	  To test with fstests one must set the TEST_DEV variable, this sets
	  the TEST_DEV to use.

if FSTESTS_XFS_SECTION_LOGDEV_ENABLED

config FSTESTS_TEST_LOGDEV
	string "The device to use TEST_LOGDEV on fstests"
	default "/dev/loop13" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	help
	  To test with fstests with an external journal you can set this to
	  device which will be used for the journal.

config FSTESTS_TEST_LOGDEV_MKFS_OPTS
	string "The extra mkfs options to use for the external journal"
	default "-lsize=1g"
	help
	  This sets the mkfs options to configure the external journal.

endif

config FSTESTS_TEST_DEV_ZNS
	string "The device to use TEST_DEV on fstests for ZNS testing"
	depends on LIBVIRT_ENABLE_ZNS
	default "/dev/nvme4n1" if QEMU_ENABLE_NVME_ZNS
	help
	  To test with fstests one must set the TEST_DEV variable, this sets
	  the TEST_DEV to use when testing with ZNS drives.

config FSTESTS_TEST_DIR
	string "The path to use for the fstests TEST_DIR variable"
	default "/media/test"
	help
	  To test with fstests one must set the TEST_DEV variable, this sets
	  the TEST_DEV to use.

	# default $(shell, for i in $(seq 5 11) echo /dev/loop$i) if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
config FSTESTS_SCRATCH_DEV_POOL
	string "The list of devices to set in SCRATCH_DEV_POOL"
	default "/dev/loop5 /dev/loop6 /dev/loop7 /dev/loop8 /dev/loop9 /dev/loop10 /dev/loop11 /dev/loop12" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	help
	  To set of spare devices to use for the fstests SCRATCH_DEV_POOL.

config FSTESTS_SCRATCH_DEV_POOL_ZNS
	string "The list of devices to set in SCRATCH_DEV_POOL for ZNS testing"
	depends on LIBVIRT_ENABLE_ZNS
	default "/dev/nvme5n1 /dev/nvme6n1 /dev/nvme7n1 /dev/nvme8n1 /dev/nvme9n1 /dev/nvme10n1 /dev/nvme11n1 /dev/nvme12n1" if QEMU_ENABLE_NVME_ZNS
	help
	  To set of spare devices to use for the fstests SCRATCH_DEV_POOL for ZNS testing.

config FSTESTS_SCRATCH_MNT
	string "The path to use for the fstests SCRATCH_MNT variable"
	default "/media/scratch"
	help
	  This sets the SCRATCH_MNT variable.

config FSTESTS_LOGWRITES_DEV
	string "The fstests LOGWRITES_DEV to use for powerfailing tests"
	default "/dev/loop15" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	help
	  To set devices to use for the LOGWRITES_DEV, this is used for
	  power failing tests. This requires and uses the device mapper
	  target log-writes (Linux CONFIG_DM_LOG_WRITES, the target is
	  dm-log-writes). This target is used to write log all write
	  operations done to the real device. This is for use by file
	  system developers wishing to verify that their fs is writing a
	  consistent file system at all times by allowing them to replay the
	  log in a variety of ways and to check the contents.

config FSTESTS_SCRATCH_LOGDEV
	string "The fstests SCRATCH_LOGDEV to use"
	default "/dev/loop15" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	help
	  To set devices to use for the SCRATCH_LOGDEV.

if FSTESTS_XFS

config FSTESTS_TEST_RTDEV
	string "The fstests TEST_RTDEV to use"
	default "/dev/loop13" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_XFS_SECTION_RTDEV_ENABLED
	help
	  To set devices to use for the TEST_RTDEV.

config FSTESTS_SCRATCH_RTDEV
	string "The fstests SCRATCH_RTDEV to use"
	default "/dev/loop14" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_XFS_SECTION_RTDEV_ENABLED
	help
	  To set devices to use for the SCRATCH_RTDEV.

config FSTESTS_TEST_RTLOGDEV_RTDEV
	string "The fstests TEST_RTDEV to use for XFS when both logdev and rtdev are required"
	default "/dev/loop12" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_XFS_SECTION_RTDEV_ENABLED
	depends on FSTESTS_XFS_SECTION_LOGDEV_ENABLED
	help
	  To set devices to use for the TEST_RTDEV for the XFS target test
	  [xfs_rtlogdev]. This uses both an external journal device and a
	  realtime device.

config FSTESTS_SCRATCH_DEV_POOL_RTLOGDEV
	string "The list of devices to set in SCRATCH_DEV_POOL used for the XFS [xfs_rtlogdev]"
	default "/dev/loop5 /dev/loop6 /dev/loop7 /dev/loop8 /dev/loop9 /dev/loop10 /dev/loop11" if FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_TESTDEV_SPARSEFILE_GENERATION
	depends on FSTESTS_XFS_SECTION_RTDEV_ENABLED
	depends on FSTESTS_XFS_SECTION_LOGDEV_ENABLED
	help
	  To set of spare devices to use for the fstests SCRATCH_DEV_POOL for
	  testing with the XFS [xfs_rtlogdev].

endif # FSTESTS_XFS

config FSTESTS_SETUP_SYSTEM
	bool "Help setup system if something is found missing or broken"
	default y
	help
	  This adds adds users / groups if missing. Additionally if
	  a service is not avaiable it ensures to start it. These quirks should
	  all be moved as part of the ansible role for fstests eventually.

config FSTESTS_RUN_TESTS
	bool "Run fstests"
	default y
	help
	  Disable this to only install dependencies for fstests, and so the
	  full set of fstests won't be run.

config FSTESTS_RUN_AUTO_GROUP_TESTS
	bool "Run fstests of group 'auto'"
	depends on FSTESTS_RUN_TESTS
	depends on !FSTESTS_ENABLE_RUN_CUSTOM_TESTS
	default y
	help
	  The 'auto' test group is a good choice for running regression tests.
	  Disable this to run also the tests that are not in the 'auto' group.

config FSTESTS_RUN_CUSTOM_GROUP_TESTS
	string "Run a custom group of fstests"
	default "auto" if FSTESTS_RUN_AUTO_GROUP_TESTS
	depends on FSTESTS_RUN_TESTS
	depends on !FSTESTS_ENABLE_RUN_CUSTOM_TESTS
	help
	  To configure fstests to run specific group of tests instead of the
	  'auto' group.

config FSTESTS_EXCLUDE_TEST_GROUPS
	string "Comma separated list of groups to be excluded"
	default ""
	depends on FSTESTS_RUN_TESTS
	help
	   List of test groups to be excluded from being executed.

config FSTESTS_ENABLE_RUN_CUSTOM_TESTS
	bool "Enable running custom tests"
	depends on FSTESTS_RUN_TESTS
	default n
	help
	  Enable this only if you want to always run custom tests.
	  In the future we will support the ability to dynamically run
	  custom tests at runtime, and so this option could eventually
	  become deprecated because we typically don't want to run custom
	  tests for a setup every time, only sporadically. This option
	  exists to ensure that if you enable custom tests for the setup
	  you cannot enable FSTESTS_RUN_AUTO_GROUP_TESTS.

if FSTESTS_ENABLE_RUN_CUSTOM_TESTS

config FSTESTS_RUN_CUSTOM_TESTS
	string "Run a custom set of fstests"
	depends on FSTESTS_RUN_TESTS
	help
	  To configure fstests to run specific set of tests/groups.
	  When FSTESTS_RUN_AUTO_GROUP_TESTS is enabled, the custom test are run
	  in addition to the auto group tests. To run only the custom tests,
	  disable FSTESTS_RUN_AUTO_GROUP_TESTS. The custom tests do not override
	  expunge lists. If a custom test is listed in expunge list, it will not
	  run. Similarly, if a custom test is listed as a large disk test, it
	  will not run if FSTESTS_RUN_LARGE_DISK_TESTS is disabled.

endif

config FSTESTS_RUN_LARGE_DISK_TESTS
	bool "Run fstests that require large disk"
	depends on FSTESTS_RUN_TESTS
	default y
	help
	  Disable this to avoid running tests that require a "large disk"
	  to run. The "large disk" requirement is test dependent, but
	  typically, it means a disk with capacity of at several 10G.

config FSTESTS_SOAK_DURATION
	int "Custom Soak duration to be used"
	default 0
	help
	  Custom Soak duration to be used during test execution. If you set this
	  to a non-zero value then fstests will increase the amount of time it
	  takes to run certain tests which are time based and support using
	  SOAK_DURATION. A moderate high value setting for this is 9900 which is
	  2.5 hours.

	  As of 2023-10-31 that consists of the following tests which use either
	  fsstress or fsx or fio. Tests either use SOAK_DURATION directly or they
	  use the helper loop such as _soak_loop_running().

	  - generic/019 - fsstress + fio + disk-failure-simulation|dio/aio/mmap
	  - generic/476 - fsstress - multithreaded write
	  - generic/388 - fsstress + shutdowns after log recovery
	  - generic/475 - fsstress + disk failures on scratch device
	  - generic/521 - fsx - direct IO
	  - generic/522 - fsx - buffered IO
	  - generic/616 - fsx - iouring (-U) buffered IO
	  - generic/617 - fsx - iouring (-U) direct IO
	  - generic/642 - fsstress - multithreaded xattr testing
	  - generic/648 - fsstress + disk failures on loopback
	  - generic/650 - fsstress - multithreaded write + CPU hotplug

	  The tests below use _scratch_xfs_stress_scrub() to stress
	  test an with fsstress with scrub or an alternate xfs_db operation.

	  - xfs/285
	  - xfs/517
	  - xfs/560
	  - xfs/561
	  - xfs/562
	  - xfs/565
	  - xfs/570
	  - xfs/571
	  - xfs/572
	  - xfs/573
	  - xfs/574
	  - xfs/575
	  - xfs/576
	  - xfs/577
	  - xfs/578
	  - xfs/579
	  - xfs/580
	  - xfs/581
	  - xfs/582
	  - xfs/583
	  - xfs/584
	  - xfs/585
	  - xfs/586
	  - xfs/587
	  - xfs/588
	  - xfs/589
	  - xfs/590
	  - xfs/591
	  - xfs/592
	  - xfs/593
	  - xfs/594
	  - xfs/595
	  - xfs/727
	  - xfs/729
	  - xfs/800

endif # KDEVOPS_WORKFLOW_ENABLE_FSTESTS
